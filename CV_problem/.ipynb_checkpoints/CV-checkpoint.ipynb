{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 784)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This script train a model to perform number classification into 4 classes.\n",
    "CNN was picked to performing trainging since they can be scaled to higer resolutions too\n",
    "and most importantly have a higher accuracy than their counterparts.'''\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''First Intitutions: Unpickle the test/test image objects and save them\n",
    "locally for further training of ML or DL Model'''\n",
    "label_pickle = open(\"train_label.pkl\",\"rb\")\n",
    "labels=pickle.load(label_pickle)\n",
    "with open('test_image.pkl','rb') as file:\n",
    "    test_image=pickle.load(file)\n",
    "\n",
    "with open('train_image.pkl','rb') as file:\n",
    "    train_image=pickle.load(file)\n",
    "#print(type(test_image))\n",
    "#print(type(train_image))    \n",
    "#print(type(labels))\n",
    "'''On examining the unpickled objects, it was known that their types were lists\n",
    "'''\n",
    "#print(len(test_image))\n",
    "#print(len(train_image))    \n",
    "#print(len(labels))\n",
    "#On examining the length it was known that there were 8000 samples of numbers images\n",
    "#print(test_image)\n",
    "# Each element in the list had 784 items, hence after looking upthe internet it was known \n",
    "#that the values were 0 to 255 grey scale.\n",
    "# By intuition we could imagine a square pixel image of 28*28 pixel image \n",
    "#By taking this intiution forward , we proceed to reshape the np array into 28,28 to \n",
    "#eventually feed into out CNN model for training images.\n",
    "\n",
    "test_image_np=np.array(test_image)\n",
    "train_image_np=np.array(train_image)\n",
    "#The lists are converted into np array for test train splitting and entutally input into \n",
    "# the models\n",
    "# print(test_image.shape)\n",
    "train_shuffle,labels_shuffle = shuffle(train_image_np, labels, random_state=42)\n",
    "train_shuffle,labels_shuffle = shuffle(train_shuffle, labels_shuffle, random_state=38)\n",
    "'''provided train data is shuffled for better training and accuracy by sklearn's shuffle method.\n",
    "We then spilit the train and test images into 9:1 ratio for optimum model accurracy \n",
    "as the sample size of the dataset is less'''\n",
    "\n",
    "x_train=np.array(train_shuffle[0:7200])\n",
    "y_train=np.array(labels_shuffle[0:7200])\n",
    "x_test=np.array(train_shuffle[7200:8000])\n",
    "y_test=np.array(labels_shuffle[7200:8000])\n",
    "x_train_al=x_train\n",
    "y_train_al=y_train\n",
    "x_train.shape\n",
    "# test_image_np = test_image_np.reshape(x_train.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7200, 28, 28, 1)\n",
      "Number of images in x_train 7200\n",
      "Number of images in x_test 800\n"
     ]
    }
   ],
   "source": [
    "'''In Keras API, we need 4-dims numpy arrays but our array is 3-dimensional.\n",
    "In addition, we must normalize our data as required in neural network models by diving its \n",
    "float value by the maxaimum RGB value(grey scale) by its maximum value 255'''\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "#reshaping the np arrays\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "\n",
    "x_train /= 255\n",
    "\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.6360 - acc: 0.7531\n",
      "Epoch 2/12\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.4526 - acc: 0.8229\n",
      "Epoch 3/12\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.3952 - acc: 0.8468\n",
      "Epoch 4/12\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.3484 - acc: 0.8633\n",
      "Epoch 5/12\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.3082 - acc: 0.8831\n",
      "Epoch 6/12\n",
      "7200/7200 [==============================] - 15s 2ms/step - loss: 0.2605 - acc: 0.8996\n",
      "Epoch 7/12\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2379 - acc: 0.9108\n",
      "Epoch 8/12\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.2101 - acc: 0.9212\n",
      "Epoch 9/12\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.1737 - acc: 0.9406\n",
      "Epoch 10/12\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1513 - acc: 0.9474\n",
      "Epoch 11/12\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1257 - acc: 0.9551\n",
      "Epoch 12/12\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.1123 - acc: 0.9625: 4s - loss: 0.11 - ETA: 0s - loss: 0.1119 - ac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c81b9e5940>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building the CNN\n",
    "'''We will build our model by using high level Keras API \n",
    "which uses either TensorFlow or Theano on the backend'''\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "# Creating a Sequential Model and adding the layers\n",
    "\n",
    "model = Sequential()\n",
    "'''Convolutional layer preserve the relationship between different parts of an image,\n",
    "filtering the image with a smaller pixel filter \n",
    "to decrease the size of the image without loosing the relationship between pixels.'''\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "#  Pooling layer reduce the spatial size of the representation to reduce the parameter\n",
    "# counts which reduces the computational complexity\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "#In addition, Dropout layers fight with the overfitting by disregarding some of the neurons while training while \n",
    "# Flatten layers flatten 2D arrays to 1D array before building the fully connected layers.\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We have to optimize with a given loss function which uses a metric. Then, we can fit the model\n",
    "# by using the train data.\n",
    "model.compile(optimizer='adam', \n",
    "\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39630293875932693, 0.8625]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)\n",
    "#We finally evalaute the model to chcck for its accuracy against out test data and labels provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Sample prediction with test data \n",
    "image_index = 266\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())\n",
    "print(y_test[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This script will predict the numbers into their respective classes and then add them into \n",
    "a csv file.'''\n",
    "import csv\n",
    "\n",
    "# csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "csvdata=[['image_index','class']]\n",
    "#Initial header of the csv file\n",
    "'''the list items containing the test_image_index and the predicted class for \n",
    "the reshaped test_image_np is appened onyto the csvdata list'''\n",
    "for image_index in range(0,len(test_image_np)):\n",
    "    csvdata.append([image_index,model.predict(test_image_np[image_index].reshape(1, 28, 28, 1)).argmax()])\n",
    "    \n",
    "with open('RAM PRASAD E NAIK.csv', 'w',newline='') as csvFile:        \n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvdata)\n",
    "csvFile.close()\n",
    "'''A new file is opened and csv writer() is used to write the rows of data held by the csvdata \n",
    "list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
